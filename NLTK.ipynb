{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35b2c6-26c7-4f3a-9bf4-fa931375fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72bd7b9-f178-422d-b5ee-de0a9b3b13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenisation - breaking of sentence to words and sentences to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e907beb-8f53-47d8-a0dd-a1afd57d8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124567d3-557d-4595-8c94-b0039305b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c1c71-3077-4a26-a526-416b6ede3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store and tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022fe6d1-ff99-4efe-ab21-b0e86b99336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"Hey there!! This is my debut nltk work.\n",
    "Hope we enjoy learninnnn....!!!!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb460787-e295-45c2-abf9-b42312fd203e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey there!!',\n",
       " 'This is my debut nltk work.',\n",
       " 'Hope we enjoy learninnnn....!!!!!',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c1778-e84f-4e76-911f-26664d562ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets store and tokenize a sentence to words. I will be using the second sentence from the above tokenized one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3459ff0e-9aa2-40d0-8a7f-dc37e9d8e642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'my', 'debut', 'nltk', 'work', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = sent_tokenize(sentences)[1]\n",
    "word_tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de278fc-6fb0-4aec-ac33-f49b13ca3721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c0b5c7-224d-4465-9533-5f83284e9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lemmatization - reducing to base/root form of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c2a11b5-1f86-4578-bb72-f33257c4e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The friends of DeSoto love scarves\"\n",
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6c8f0-7dc4-4c5a-bd40-e2cd1511d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and creating an object for the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db0e6e5f-ff8c-40ba-874c-61b8d0ab0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b2b384-98de-4666-a988-b5dfa7a5a163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashis\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b60b9603-ea69-492b-8dc6-dfe225fb2da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(wordss) for wordss in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e101e3-baca-4704-a38e-af401c9c8e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3e4616d-7793-49ca-b803-e6d94946a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stemming - removing the suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aed41fd-b460-4431-bf0a-504bd9f65f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ed3f0d9-36df-499b-9cd3-05f0f77a49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"So am gonna try stemming but don't fall trying!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d413c0d5-c09c-45f8-a569-78f8e511ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_tokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41fceac8-8fd9-43d9-b8c0-7eb240e63d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so',\n",
       " 'am',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'tri',\n",
       " 'stem',\n",
       " 'but',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'fall',\n",
       " 'tri',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(words) for words in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ef2f5-53a6-4321-911d-b0e90f2bdf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66a957b2-3f4a-4e06-a535-d34516d9d483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Removing stop words - words like is,the,an etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "363ce165-1411-495d-a9d1-3e65f67e6506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6be4caab-adb0-4222-8826-c27e6882c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I think this is an intresting topic to work on, ain't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfd9e470-bb4f-4ff7-95ad-0dac68257dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139309e-d52d-4097-8afe-fde1943dddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be removing words that not stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a166ce8-4046-4a25-bd16-a5505d2b317d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'think', 'intresting', 'topic', 'work', ',', 'ai', \"n't\", '?']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [words for words in word if words not in stop_words]\n",
    "filtered_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
