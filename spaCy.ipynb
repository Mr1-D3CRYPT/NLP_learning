{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9257818b-ea73-4e1d-80e6-d4ca028795a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e33a348e-3708-4f73-ab34-aaee100a446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e491d07-49e7-4b20-8c7b-b1530437bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f0fb745-bfa5-413d-91b3-36d8db60a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "words = nlp(\"Hey aliens!! Let's spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "498dd148-782e-477b-ba16-2bedb9094c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey -> hey\n",
      "aliens -> alien\n",
      "! -> !\n",
      "! -> !\n",
      "Let -> let\n",
      "'s -> us\n",
      "spacy -> spacy\n"
     ]
    }
   ],
   "source": [
    "for tokens in words:\n",
    "    print(tokens.i, tokens.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e90f9-26e8-4ae9-8419-e233d4deeb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f32860c-1792-4d95-9b01-488cef382696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "241fa389-3afd-420a-a213-9def49028018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey -> hey\n",
      "aliens -> alien\n",
      "! -> !\n",
      "! -> !\n",
      "Let -> let\n",
      "'s -> us\n",
      "spacy -> spacy\n"
     ]
    }
   ],
   "source": [
    "for tokens in words:\n",
    "    print(tokens.text , \"->\" , tokens.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c603e0-df36-4fbd-87af-3d0f16853603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e7dddfe-e9fa-45e5-b448-37fd153f792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stemming - unfortunately spaCy doesnot support stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dad126-eb12-40a4-b446-08f1ffd03a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbecfb7e-5399-42c3-a052-cb2668da854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9bdc5c5-d4f7-4200-8d73-afc1045836b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"So i was trying to remove is and the in spacy which meant stop words\"\n",
    "doc = nlp(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99cab5e3-1826-485a-9b63-04900301abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [token.text for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79108375-cbb3-4d1d-ae43-0ecdc6b8c84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trying', 'remove', 'spacy', 'meant', 'stop', 'words']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
